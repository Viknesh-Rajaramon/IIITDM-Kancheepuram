# -*- coding: utf-8 -*-
"""Q5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r7uLIUmgVuRWhm2a1f8-l5CmpSOSESjY

# **Q5. Fisherfaces - Face classification using LDA (40 classes)**
> **a) Use the following "face.csv" file to classify the faces of 40 different people.**

> **b) Do not use in-built function for implementing LDA.**

> **c) Use appropriate classifier taught in class (any classification algorithm taught in class like Bayes classifier, minimum distance clasifier, and so on)**

> **d) Refer to the following link for a description of the dataset**

> **https://towardsdatascience.com/eigenfaces-face-classification-in-python-7b8d2af3d3ea**

### Importing the necessary libraries
"""

import numpy as np
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import scikitplot as skplt

"""### Importing the dataset"""

df = pd.read_csv("face.csv")
df.head()

df.shape

"""### LDA function"""

def LDA(X,labels):

    d = X.shape[1]
    classes = np.unique(labels)
    c = len(classes)
    d_= c - 1
    class_dict = {}
    for i in range(len(classes)):
        class_dict[classes[i]] = i

    class_wise_data = [np.empty((0,)+X[0].shape,float) for i in classes]
    for i in range(len(X)):
        class_wise_data[class_dict[labels[i]]] = np.append(class_wise_data[class_dict[labels[i]]], np.array([X[i],]),axis=0)

    means = []
    for i in class_wise_data:
        means.append(np.mean(i,axis = 0))

    Sw = np.zeros((d,d))
    for i,data in enumerate(class_wise_data):
        z = data-means[i]
        Sw += (z.T @ z)
    Sw_inv = np.linalg.inv(Sw)

    overall_mean = np.mean(X,axis=0)
    Sb = np.zeros((d,d))
    for i, data in enumerate(means):
        Ni = len(class_wise_data[i])
        z = np.array([means[i]-overall_mean])
        Sb += (Ni * (z.T @ z))

    M = Sw_inv @ Sb
    eigen_values , eigen_vectors = np.linalg.eigh(M)
    sorted_index = np.argsort(eigen_values)[::-1]
    sorted_eigenvectors = eigen_vectors[:,sorted_index]
    sorted_eigenvalue = eigen_values[sorted_index]
    eigenvector_subset = sorted_eigenvectors[:,0:d_]

    plt.bar(list(range(1,eigen_vectors.shape[0]+1)),sorted_eigenvalue)
    plt.ylabel("eigen values")

    Y = X @ eigenvector_subset
    return Y,eigenvector_subset

"""### Preprocessing"""

X = df.iloc[:,:-1]
target = df.iloc[:,-1]

"""### Test and Train Split"""

train_data = pd.concat([df.iloc[i*10+2:(i+1)*10] for i in range(40)])
test_data = pd.concat([df.iloc[i*10:i*10+2] for i in range(40)])
train_data.reset_index(drop=True, inplace=True)
test_data.reset_index(drop=True, inplace=True)

"""### Eigen Vectors"""

reduced, eigen_vec_subset = LDA(np.array(train_data.iloc[:,:-1]),list(train_data['target']))
reduced = pd.DataFrame(reduced)

model = GaussianNB()
model.fit(reduced,train_data["target"])

test_reduced = (test_data.iloc[:,:-1]).dot(eigen_vec_subset)
predicted = model.predict(test_reduced)
test_reduced['target'] = test_data['target']
test_reduced['predicted'] = predicted
correctness = []

for i in test_reduced.index:
    if test_reduced['target'][i] == test_reduced['predicted'][i]:
        correctness.append("correct")
    else:
        correctness.append("wrong")

test_reduced["correctness"] = correctness
print(test_reduced)

"""### Accuracy"""

x = accuracy_score(test_reduced["target"],predicted)
print(f"Accuracy = {x*100}%")

skplt.metrics.plot_confusion_matrix(test_reduced["target"],predicted)
plt.xlabel('Predicted Class')
plt.ylabel('Actual Class')
plt.show()