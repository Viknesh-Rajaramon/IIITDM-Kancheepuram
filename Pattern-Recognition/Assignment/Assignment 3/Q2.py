# -*- coding: utf-8 -*-
"""PR_Assignment3_Q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wdeAR6MOE64Io72LeV1QRD7TCpab0Dgf

# **Q2. For the same dataset (2 classes, male and female)**

> **a) Use LDA to reduce the dimension from d to d’. (Here d=128)**

> **b) Choose the direction W to reduce the dimension d' (select appropriate d').**

> **c) Use d’ features to classify the test cases (any classification algorithm will do, Bayes classifier, minimum distance classifier, and so on)**

### importing the necessary libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""
from google.colab import files
uploaded = files.upload()
"""

df=pd.read_csv("gender_feature_vectors.csv")
df.head()

df.shape

df.drop(['Unnamed: 0'],axis=1,inplace=True)

df.head()

df2=df.drop(['Unnamed: 1'],axis=1)
Class=np.array(df2)
print(Class)

class_wise=df.groupby('Unnamed: 1')

class_wise.head()

"""### splitting the dataset wrt to both the classes"""

class0=Class[:399]
class1=Class[399:]
# class0.head()
print(class0)

print(class1)

"""## LDA"""

def lda(class0,class1,d_prime):
    mean0=np.average(class0, axis=0)
    mean1=np.average(class1, axis=0)
#     print(mean0)
#     print(mean1)
    Sw = np.zeros((len(mean0), len(mean0)))

    for row in class0:
        subbed = (row - mean0).reshape((len(mean0), 1))
        dotted = np.dot(subbed, subbed.T)
        Sw += dotted    

    for row in class1:
        subbed = (row - mean1).reshape((len(mean1), 1))
        dotted = np.dot(subbed, subbed.T)
        Sw += dotted  
        
    Sb = np.zeros((len(mean0), len(mean0)))
    subbed = (mean0 - np.average(Class, axis = 0)).reshape((len(mean0), 1))
    Sb += len(class0) * np.dot(subbed, subbed.T)

    subbed = (mean1 - np.average(Class, axis = 0)).reshape((len(mean1), 1))
    Sb += len(class1) * np.dot(subbed, subbed.T)

    # finding eigen values and eigen vectors for Sw^-1*Sb.
    e_val, e_vec = np.linalg.eigh(np.dot(np.linalg.inv(Sw), Sb))

    sorted_e_val = np.flip(np.sort(e_val))
    sorted_e_vec = e_vec.copy()
    dummy = 0

    #mapping the eigen vectors to the corresponding sorted eigen values
    for val in sorted_e_val:
        ind = np.argmax(e_val == val * 1)
        sorted_e_vec[:,dummy] = e_vec[:,ind]
        dummy +=1

    
    
    new_feat = np.dot(Class, sorted_e_vec)

    return new_feat[:,:d_prime]

# lda(class0,class1,50)

lda_model=lda(class0,class1,1)

male=lda_model[:399]
female=lda_model[399:]
male

female

"""## Train test split"""

X_train=male[10:]
X_test=male[:10]
Y_train,Y_test=female[10:],female[:10]

X_train

"""### plot to show clustering"""

plt.figure(figsize= (6, 6))

plt.scatter(X_train[:,0], range(len(X_train[:,0])))
plt.scatter(Y_train[:,0], range(len(Y_train[:,0])))

plt.scatter(Y_test[:,0], range(len(Y_test[:,0])))
plt.scatter(X_test[:,0], range(len(X_test[:,0])))

plt.legend(['X_train', 'Y_train', 'Y_test', 'X_test'])
plt.show()

"""### minimum distance classifier"""

print('Predictions for the male test points:\n')

prediction1 = []
for samp in X_test:
    prediction1.append(np.sqrt(np.sum(np.square(X_train - samp))) < np.sqrt(np.sum(np.square(Y_train - samp))))
    print('The prediction is', np.sqrt(np.sum(np.square(X_train - samp))) < np.sqrt(np.sum(np.square(Y_train - samp))))

prediction1 = np.stack(prediction1) * 1 #this is to convert the boolean to numbers

print('\n')
print('\nPredictions for the female test points:\n')

prediction2 = []
for samp in Y_test:
    prediction2.append(np.sqrt(np.sum(np.square(X_train - samp))) > np.sqrt(np.sum(np.square(Y_train - samp))))
    print('The predition is', np.sqrt(np.sum(np.square(X_train - samp))) > np.sqrt(np.sum(np.square(Y_train - samp))))

prediction2 = np.stack(prediction2) * 1

indices_one = prediction2 == 1
indices_zero = prediction2 == 0
prediction2[indices_one] = 0
prediction2[indices_zero] = 1

"""### Confusion Matrix"""

def confusion_matrix(pred, true):
    
    true_pos = 0
    true_neg = 0
    false_pos = 0
    false_neg = 0

    for i in range(len(pred)):
        
        if pred[i] == true[i]:
         
            if pred[i] == 1:
                true_pos += 1
            else:
                true_neg += 1
        else:

            if pred[i] == 1 and true[i] == 0:
                false_pos += 1
            else:
                false_neg += 1

    return true_pos, true_neg, false_pos, false_neg

"""### Using different performance metrics"""

predictions=np.concatenate((prediction1,prediction2),axis=0)
x1=np.ones(len(prediction1))
x2=np.zeros(len(prediction2))
x=np.concatenate((x1,x2),axis=0)
true_positives,true_negatives,false_positives,false_negatives=confusion_matrix(predictions,x)

accuracy = (true_positives + true_negatives)/(true_positives + true_negatives + false_positives + false_negatives)
precision = (true_positives)/(true_positives + false_positives)
recall = (true_positives)/(true_positives + false_negatives)
f1_score = 2 * precision * recall / (precision + recall)

print('Accuracy =', accuracy)
print('Precision =', precision)
print('Recall = ', recall)
print('F1 Score = ', f1_score)

class_names = ['Female', 'Male']
  
# randomly generated array
#random_array = np.random.random((2, 2))

matrix = np.array([[true_negatives, false_positives], [false_negatives, true_positives]])

figure = plt.figure()
axes = figure.add_subplot(111)
  
# using the matshow() function 
caxes = axes.matshow(matrix, interpolation ='nearest', cmap = plt.cm.Blues)
figure.colorbar(caxes)
  
axes.set_xticklabels([''] + class_names)
axes.set_yticklabels([''] + class_names)

plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.gca().xaxis.tick_bottom()

for i in range(len(matrix)):
  for j in range(len(matrix)):
    plt.text(j, i, str(matrix[i][j]), va = 'center', ha = 'center')
  
plt.title('Confusion Matrix')
plt.show()